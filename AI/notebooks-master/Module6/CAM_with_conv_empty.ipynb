{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZBwDI7xiVu5"
   },
   "source": [
    "# Class Activation Map with convolutions\n",
    "\n",
    "In this exercice, we will code class activation map as described in the paper [Learning Deep Features for Discriminative Localization](http://cnnlocalization.csail.mit.edu/)\n",
    "\n",
    "There is a GitHub repo associated with the paper:\n",
    "https://github.com/zhoubolei/CAM\n",
    "\n",
    "And even a demo in PyTorch:\n",
    "https://github.com/zhoubolei/CAM/blob/master/pytorch_CAM.py\n",
    "\n",
    "The code below is adapted from this demo but we will not use hooks only convolutions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNUU0eixiVvB"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "from IPython.display import Image as Img\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pdb\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "\n",
    "# input image\n",
    "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
    "IMG_URL = 'http://media.mlive.com/news_impact/photo/9933031-large.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the demo, we will use the Resnet18 architecture. In order to get CAM, we need to transform this network in a fully convolutional network: at all layers, we need to deal with images, i.e. with a shape $\\text{Number of channels} \\times W\\times H$ . In particular, we are interested in the last images as shown here:\n",
    "![](https://camo.githubusercontent.com/fb9a2d0813e5d530f49fa074c378cf83959346f7/687474703a2f2f636e6e6c6f63616c697a6174696f6e2e637361696c2e6d69742e6564752f6672616d65776f726b2e6a7067)\n",
    "\n",
    "As we deal with a Resnet18 architecture, the image obtained before applying the `AdaptiveAvgPool2d` has size $512\\times 7 \\times 7$ if the input has size $3\\times 224\\times 224 $:\n",
    "![resnet_Archi](https://pytorch.org/assets/images/resnet.png)\n",
    "\n",
    "1- The first thing, you will need to do is 'removing' the last layers of the resnet18 model which are called `(avgpool)` and `(fc)`. Check that for an original image of size $3\\times 224\\times 224 $, you obtain an image of size $512\\times 7\\times 7$.\n",
    "\n",
    "2- Then you need to retrieve the weights (and bias) of the `fc` layer, i.e. a matrix of size $1000\\times 512$ transforming a vector of size 512 into a vector of size 1000 to make the prediction. Then you need to use these weights and bias to apply it pixelwise in order to transform your $512\\times 7\\times 7$ image into a $1000\\times 7\\times 7$ output (Hint: use a convolution). You can interpret this output as follow `output[i,j,k]` is the logit for 'pixel' `[j,k]` for being of class `i`.\n",
    "\n",
    "3- From this $1000\\times 7\\times 7$ output, you can retrieve the original output given by the `resnet18` by using an `AdaptiveAvgPool2d`.\n",
    "\n",
    "4- In addition, you can construct the Class Activation Map. Draw the activation map for the classe mountain bike, for the classe lake.\n",
    "\n",
    "5- Is your network working on an image which is not of size $224\\times 224$? and what about `resnet18`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cMgoA17fiVvT"
   },
   "outputs": [],
   "source": [
    "net = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 3, 224, 224)\n",
    "y = net(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quQMxt1-iVxL"
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   #transforms.Resize((224,224)),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n",
    "\n",
    "response = requests.get(IMG_URL)\n",
    "img_pil = Image.open(io.BytesIO(response.content))\n",
    "img_pil.save('test.jpg')\n",
    "imshow(img_pil);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = preprocess(img_pil)\n",
    "net.eval()\n",
    "logit = net(img_tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the imagenet category list\n",
    "classes = {int(key):value for (key, value)\n",
    "          in requests.get(LABELS_URL).json().items()}\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.numpy()\n",
    "idx = idx.numpy()\n",
    "# output the prediction\n",
    "for i in range(0, 5):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnCAM(feature_conv, idx):\n",
    "    # input: tensor feature_conv of dim 1000*W*H and idx between 0 and 999\n",
    "    # output: image W*H with entries rescaled between 0 and 255 for the display\n",
    "    cam = feature_conv[idx].detach().numpy()\n",
    "    cam = cam - np.min(cam)\n",
    "    cam_img = cam / np.max(cam)\n",
    "    cam_img = np.uint8(255 * cam_img)\n",
    "    return cam_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_CAM = returnCAM(torch.eye(7).unsqueeze(0),0)\n",
    "img = cv2.imread('test.jpg')\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(diag_CAM,(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('diag_CAM.jpg', result)\n",
    "Img('diag_CAM.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "CAM_colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "dldiy",
   "language": "python",
   "name": "dldiy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
